{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is basics tutorial on GYM - a Reinforcement Learning framework with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-22 12:18:51,866] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Get your environment\"\"\"\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about environments, visits:  <br>\n",
    "https://gym.openai.com/envs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For reinforcement learning, we're facing a scenario that we do not have any information about future reward or the transistion function T(s,a,s'), thus it is important to get samples (episodes)** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03443529, -0.0317131 , -0.01125142,  0.04994234])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put ourselves in start states\n",
    "#also return a state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder what do the states represent? Let's look at the images:<br>\n",
    "<img src=\"cartpole.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >**System (short) description** </font> : a pole is attached to a cart, when the cart moves, the pole will have a swinging motion follow, our input to the cart is either force, accleration,etc... For more details about the system and what we are about to do, I encourage you to visit this link: https://www.youtube.com/watch?v=Lt-KLtkDlh8  <br>\n",
    "\n",
    "<font color = blue >**Goal (short) description** </font> : Keep the pole stand upward by applying correct force to the cart. <br>\n",
    "\n",
    "For the state we've been shown, here's a details: <br>\n",
    "index: 0 - cart position | 1 - cart velocity | 2 - pole angle | 3 - pole velocity <br>\n",
    "**For almost dynamical systems, position and velocity are defined as the state of them.**\n",
    "What about acceleration? For short and simple answer, dynamical systems are often represented with the following differential equations, given state vector (postion, velocity) q and input (force, acceleration, etc...) u: <br>\n",
    "\\begin{align}\n",
    "\\dot{q} & = A(q,u) \\\\\n",
    "\\end{align}\n",
    "\n",
    "If we recall, derivative of velocity of acceleration, and that is represented in the RHS of our equation. <br>\n",
    "\n",
    "For the control theory (if you known something about this), we know the function dynamics function A (which is somehow related to the transistion function), but in reinforcement learning scenario, little to none we have any information about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To observe, press tab after box. for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # 2 types of action: push left, push right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n # Seems like the input is not continuous...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's play an episode:\"\"\"\n",
    "obser, reward, done,info = env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03960299, -0.4216465 , -0.00347149,  0.62848651])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Let's see them\"\"\"\n",
    "obser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  [-0.03741105 -0.18824982  0.03783748  0.31403223]  actions:  1\n",
      "state:  [-0.04117604  0.00631327  0.04411813  0.03351805]  actions:  0\n",
      "state:  [-0.04104978  0.2007757   0.04478849 -0.24492523]  actions:  0\n",
      "state:  [-0.03703426  0.39523027  0.03988998 -0.52315117]  actions:  1\n",
      "state:  [-0.02912966  0.58976876  0.02942696 -0.80300223]  actions:  1\n",
      "state:  [-0.01733428  0.7844751   0.01336691 -1.08628502]  actions:  0\n",
      "state:  [-0.00164478  0.97941822 -0.00835879 -1.37474381]  actions:  0\n",
      "state:  [ 0.01794358  1.17464363 -0.03585366 -1.67002916]  actions:  1\n",
      "state:  [ 0.04143646  0.9799562  -0.06925424 -1.38872441]  actions:  1\n",
      "state:  [ 0.06103558  0.78576213 -0.09702873 -1.11847603]  actions:  1\n",
      "state:  [ 0.07675082  0.98201389 -0.11939825 -1.43995075]  actions:  0\n",
      "state:  [ 0.0963911   0.78854795 -0.14819727 -1.18683766]  actions:  1\n",
      "state:  [ 0.11216206  0.59562524 -0.17193402 -0.94403625]  actions:  1\n",
      "state:  [ 0.12407456  0.40318418 -0.19081475 -0.70993069]  actions:  1\n",
      "state:  [ 0.13213825  0.21114503 -0.20501336 -0.48285995]  actions:  0\n",
      "state:  [ 0.13636115  0.01941669 -0.21467056 -0.26114273]  actions:  0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obser, reward, done,info = env.step(env.action_space.sample())\n",
    "    print 'state: ', obser,' actions: ', action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search:\n",
    "\n",
    "Oftenly, we would like to find optimal weights to obtain the maximum (minimum) values. Now, we will not talk about gradient descent, instead, we are going to implement a random search algorithm. The pseudocode for random search for finding maxima is as follows: <br>\n",
    "\n",
    "--------------\n",
    "1. Initialize x (weights) & calculate f(x)<br>\n",
    "2. While not satisfied: <br>\n",
    "          -Randomly pick new weight values y\n",
    "          -If (f(y) > f(x))\n",
    "                set: x = y\n",
    "---------------\n",
    "\n",
    "<font color = green > **As we may figured out, doing this way is extremly inefficient (space complexity, no heuristic, etc...). However, as introduction to gym, this is a good thing to try out.** </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #our old friends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.68716042 -0.18711006  1.45110201 -1.09870933]\n"
     ]
    }
   ],
   "source": [
    "mu = 0\n",
    "sigma = 1\n",
    "w = np.random.normal(mu, sigma, 4)\n",
    "print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Our policy is simple, if the state * w >0, take action 1, else 0\n",
    "#Let's \"train\" our model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
