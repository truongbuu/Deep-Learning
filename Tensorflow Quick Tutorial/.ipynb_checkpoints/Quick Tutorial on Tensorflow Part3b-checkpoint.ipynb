{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a quick tutorial on Tensorflow based on Stanford CS 20SI\n",
    "\n",
    "Now it's time for us to train a deep learning model on MNIST dataset\n",
    "\n",
    "<font color = green> **This section will be a basebone for orther project (seriously, deep learning models are only different on model, cost function, optimizations, oops... (too much))**  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os                    # For path\n",
    "import numpy as np           # You should know this\n",
    "import time                  # For estimating running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f495b3248d0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f495b324d50>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f495b324bd0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>** As discussed in part 3a, variable scope is important for maintaining the consistency of our program**</font> <br>\n",
    "\n",
    "Let's see how that is going to use in practice, for this tutorial, it is highly recommended that **CS231n has been reviewed (we will use batch normalization, convolutional layer)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"For this lesson, we will use conv net func\"\"\"\n",
    "def conv_relu(input, kernel_shape, bias_shape):   \n",
    "    '''Create variable for convolutional layer'''\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    \n",
    "    '''Filter operation'''\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    '''These lines are for batch normalization layer'''\n",
    "    \n",
    "    mean = tf.get_variable(\"mean\", bias_shape, initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    var = tf.get_variable(\"var\", bias_shape, initializer=tf.random_uniform_initializer())\n",
    "    offset = tf.get_variable(\"off-set\", bias_shape, initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    scale = tf.get_variable(\"scale\", bias_shape, initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    \n",
    "    '''Batch norm in action'''\n",
    "    conv_norm = tf.nn.batch_normalization(tf.add(conv, biases), mean, var, offset, scale, variance_epsilon= 0.0001)\n",
    "    \n",
    "    return tf.nn.relu(conv_norm)\n",
    "\n",
    "def fully_connected(input, hidden_shape,bias_shape):\n",
    "    '''Similar...'''\n",
    "    weights = tf.get_variable(\"weights\", hidden_shape, initializer= tf.random_normal_initializer(stddev=0.5))\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    #print weights\n",
    "    mean = tf.get_variable(\"mean\", bias_shape, initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    var = tf.get_variable(\"var\", bias_shape, initializer=tf.random_uniform_initializer())\n",
    "    offset = tf.get_variable(\"off-set\", bias_shape, initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    scale = tf.get_variable(\"scale\", bias_shape, initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    fc_norm = tf.nn.batch_normalization(tf.add(tf.matmul(input, weights), biases), mean, var, offset, scale, variance_epsilon= 0.0001)\n",
    "    #fc_norm = tf.contrib.layers.batch_norm(tf.add(tf.matmul(input, weights), biases),center=True, scale=True, \n",
    "    #                                      is_training=True)\n",
    "    return tf.nn.relu(fc_norm)\n",
    "\n",
    "\n",
    "def sofmax_out(input, hidden_shape, bias_shape):\n",
    "    weights = tf.get_variable(\"weights\", hidden_shape, initializer= tf.random_normal_initializer(stddev=0.5))\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,initializer=tf.random_normal_initializer(stddev=0.5))\n",
    "    return tf.add(tf.matmul(input, weights), biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** WHY DO WE NEED TO USE RANDOM_UNIFORM FOR INITIALIZING VARIANCE? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"We've got a dataset\"\"\"\n",
    "#Step1: define input output\n",
    "with tf.name_scope(\"data\"):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name= \"pixels_input\")\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name = \"label\")\n",
    "    images = tf.reshape(x, shape=[-1, 28, 28, 1])  # <== we use images as input, not string\n",
    "#Step 2: define a model\n",
    "with tf.name_scope(\"model\"):\n",
    "    \"\"\"Now define a CNN network to classify this:\n",
    "    My proposal:\n",
    "    1./ Conv Layer: \n",
    "    2./ Pooling Layer: \n",
    "    3./ Conv Layer again: \n",
    "    4./ Pool again:\n",
    "    5./ Fully connected\n",
    "    6./ Fully connected to output\n",
    "    Each layer inside has a batch norm layer\n",
    "    Nonlinearity: relu\"\"\"\n",
    "    with tf.variable_scope(\"conv1\") as scope: # <<<<<<<<<<<<We need this line to manage our variables\n",
    "        relu1_norm = conv_relu(images, [5, 5, 1, 32], [32])\n",
    "        #relu1_norm = relu1\n",
    "        #relu1_norm = tf.layers.batch_normalization(relu1)\n",
    "    with tf.variable_scope(\"pool1\") as scope:\n",
    "        pool1 = tf.nn.max_pool(relu1_norm, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding='SAME')\n",
    "    with tf.variable_scope(\"conv2\") as scope:\n",
    "        relu2_norm = conv_relu(pool1, [5,5,32,64], [64])\n",
    "        #relu2_norm = tf.layers.batch_normalization(relu2)\n",
    "        #relu2_norm = relu2\n",
    "    with tf.variable_scope(\"pool2\") as scope:\n",
    "        pool2_norm = tf.nn.max_pool(relu2_norm, ksize=[1, 2, 2 ,1], strides =[1, 2, 2,1], padding = 'SAME')\n",
    "        #pool2_norm = tf.layers.batch_normalization(pool2)\n",
    "        #pool2_norm = pool2\n",
    "        pool2_norm = tf.reshape(pool2_norm, [-1, 7 * 7 * 64]) #before feeding to fully_connected\n",
    "    with tf.variable_scope(\"fully_connected\") as scope:\n",
    "        fc_norm = fully_connected(pool2_norm, [7*7*64, 1024], 1024)\n",
    "        #fc_norm = fc\n",
    "        #fc_norm = tf.layers.batch_normalization(fc)\n",
    "    with tf.variable_scope(\"output_layer\") as scope:\n",
    "        out = sofmax_out(fc_norm, [1024, 10], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out)\n",
    "    loss = tf.reduce_mean(entropy, name='entropy_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss/Reshape_2:0' shape=(?,) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram_loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 128\n",
    "SKIP_STEP = 40\n",
    "N_EPOCHS = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"These line of code are important for storing the stages during our training process & init variables\"\"\"\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Optimizer\") as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss\n",
    "                                        ,global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red> **Small note :** </font> <br>\n",
    "<font color = blue>\n",
    "- Our optimizer need variable global_step, this is because when we interrupt our training (or our training has finished) and we train again, optimizer needs to know where it has left in order to either skip training or continue <br>\n",
    "- global_step, as we shall see, will be updated in training as well\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Optimizer/Adam' type=AssignAdd>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/convnet_mnist/mnist-convnet-2119\n",
      "Optimization Finished!\n",
      "Total time: 19.1182291508 seconds\n",
      "Accuracy 0.9137\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #sess.run(init) #init all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter('./graphs/convnet', sess.graph)  #Skip this for now\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
    "    \n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    initial_step = global_step.eval()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        \n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op], \n",
    "                                feed_dict={x: X_batch, y:Y_batch}) \n",
    "        \n",
    "        writer.add_summary(summary, global_step=index) # '''Update the index here'''\n",
    "        \n",
    "        total_loss += loss_batch\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        loss_batch, logits_batch = sess.run([loss, out], \n",
    "                                        feed_dict={x: X_batch, y:Y_batch}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Hurray!! Done!!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
