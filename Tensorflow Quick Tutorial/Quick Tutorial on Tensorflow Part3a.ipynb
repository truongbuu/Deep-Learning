{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a quick tutorial on Tensorflow based on Stanford CS 20SI\n",
    "\n",
    "Now it's time for us to train a machine learning model\n",
    "\n",
    "<font color = green> **This section will be a basebone for orther project (seriously, deep learning models are only different on model, cost function, optimizations, oops... (too much))**  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "'''The first model will be simple Softmax regression model'''\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data #These one, well, just remember them... or copy from tensorflow website\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = purple>**MINI TUTORIAL: VARIABLE SCOPE, NAME SCOPE IN TENSORFLOW** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we will be able to implement a model y = f(x) where y is our predicted output and x is our features. I take code sample from tensorflow so that we can have a better intuition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for demo only\n"
     ]
    }
   ],
   "source": [
    "''' This is a sample code y = f(x) where x is an input images. \n",
    "---------------------------------------------------------------------------------\n",
    "\n",
    "def my_image_filter(input_images):\n",
    "    conv1_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]),      <<<<<<<<<< VARIABLES!\n",
    "        name=\"conv1_weights\")\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\")    <<<<<<<<<< VARIABLES!\n",
    "    conv1 = tf.nn.conv2d(input_images, conv1_weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "\n",
    "    conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]),       <<<<<<<<<< VARIABLES!\n",
    "        name=\"conv2_weights\")\n",
    "    conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\")     <<<<<<<<<< VARIABLES!\n",
    "    conv2 = tf.nn.conv2d(relu1, conv2_weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2 + conv2_biases)\n",
    "-----------------------------------------------------------------------------------\n",
    "'''\n",
    "print 'This is for demo only'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have 2 images, and by sometask, for example, we might want to compare 2 images after being filtered. Thus we use: <br>\n",
    "\n",
    " - result1 = my_image_filter(x1) , result2 = my_image_filter(x2) <br>\n",
    " \n",
    "<font color = green>**This will cause 1 problem, tensorflow will create 2 graph, with total 8 variables. Imagine in our deep model, each variables is a vector with a pretty large size, creating 2 graph will make us suffer from memory problem**. </font> One solution we might easily come up with is creating variables separately from the function, but this will cause encapsulation problem (just google the term if you are not familiar with it). In this tutorial, we will use **'scope'**, which is defined in Tensorflow for dealing with this problem. For more information, check this link: https://www.tensorflow.org/versions/r0.12/how_tos/variable_scope/#variable_scope_example   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** For whatever reasons, the first step is to define the dimensions of input and output***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  55000\n",
      "Dimension of each image (should be 784 1d array) 784\n",
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "print 'Number of training examples: ', mnist.train.num_examples\n",
    "print 'Dimension of each image (should be 784 1d array)',len(mnist.train.images[0])\n",
    "print 'Number of classes: ',len(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remember, for feeding inputs, we use placeholders, in this example x: input, y:output\n",
    "with tf.name_scope('input_data'):\n",
    "    x = tf.placeholder(tf.float32, shape = [None , 784]) #None means that it can be any arbitary\n",
    "    y = tf.placeholder(tf.float32, shape = [None, 10])   #Why? We will feed batch into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For softmax, define a weight matrix (and bias) first\n",
    "with tf.name_scope('Weights'):                          \n",
    "    W = tf.get_variable(\"weights\", [784,10],initializer=tf.random_normal_initializer())\n",
    "    b = tf.get_variable(\"bias\", [1,10],initializer=tf.random_normal_initializer())\n",
    "#Define our softmax output\n",
    "with tf.name_scope('model'):    \n",
    "    y_pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basically** <br>\n",
    "\n",
    "x ----> W,b ----> y_pred ---> Compute cost function ---> backprob (kind of...) --> Update W,b then feed again      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Cost_function'):\n",
    "    cost = tf.losses.softmax_cross_entropy(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Time for training our model **\n",
    "First, let see what will happen when we train our model\n",
    "- We feed batch of inputs into our model, use optimizer to minimiz the cose\n",
    "- We need to see the cost function (loss) reduce in order to make sure everything is good.\n",
    "- <font color = red> It is always necessary to save our model! </font>\n",
    "- Since mnist dataset we used has a function that generate batch for us, in other kind of data, we need to write our own batch generator function (it is easy by the way). <br>\n",
    "\n",
    "Now its time for us to train the model and see each corresponding steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10:   2.2\n",
      "Average loss at step 20:   2.0\n",
      "Average loss at step 30:   2.0\n",
      "Average loss at step 40:   1.9\n",
      "Average loss at step 50:   1.9\n",
      "Average loss at step 60:   1.9\n",
      "Average loss at step 70:   1.9\n",
      "Average loss at step 80:   1.9\n",
      "Average loss at step 90:   1.9\n",
      "Average loss at step 100:   1.9\n",
      "Average loss at step 110:   1.9\n",
      "Average loss at step 120:   1.9\n",
      "Average loss at step 130:   1.9\n",
      "Average loss at step 140:   1.9\n",
      "Average loss at step 150:   1.9\n",
      "Average loss at step 160:   1.9\n",
      "Average loss at step 170:   1.9\n",
      "Average loss at step 180:   1.9\n",
      "Average loss at step 190:   1.9\n",
      "Average loss at step 200:   1.9\n",
      "Average loss at step 210:   1.9\n",
      "Average loss at step 220:   1.9\n",
      "Average loss at step 230:   1.9\n",
      "Average loss at step 240:   1.9\n",
      "Average loss at step 250:   1.9\n",
      "Average loss at step 260:   1.9\n",
      "Average loss at step 270:   1.9\n",
      "Average loss at step 280:   1.9\n",
      "Average loss at step 290:   1.9\n",
      "Average loss at step 300:   1.9\n",
      "Average loss at step 310:   1.9\n",
      "Average loss at step 320:   1.9\n",
      "Average loss at step 330:   1.9\n",
      "Average loss at step 340:   1.9\n",
      "Average loss at step 350:   1.9\n",
      "Average loss at step 360:   1.9\n",
      "Average loss at step 370:   1.9\n",
      "Average loss at step 380:   1.9\n",
      "Average loss at step 390:   1.9\n",
      "Average loss at step 400:   1.9\n",
      "Average loss at step 410:   1.9\n",
      "Average loss at step 420:   1.9\n",
      "Average loss at step 430:   1.9\n",
      "Average loss at step 440:   1.9\n",
      "Average loss at step 450:   1.9\n",
      "Average loss at step 460:   1.9\n",
      "Average loss at step 470:   1.9\n",
      "Average loss at step 480:   1.9\n",
      "Average loss at step 490:   1.9\n",
      "Average loss at step 500:   1.9\n",
      "Average loss at step 510:   1.9\n",
      "Average loss at step 520:   1.9\n",
      "Average loss at step 530:   1.9\n",
      "Average loss at step 540:   1.9\n",
      "Average loss at step 550:   1.9\n",
      "Average loss at step 560:   1.9\n",
      "Average loss at step 570:   1.9\n",
      "Average loss at step 580:   1.9\n",
      "Average loss at step 590:   1.9\n",
      "Average loss at step 600:   1.9\n",
      "Average loss at step 610:   1.9\n",
      "Average loss at step 620:   1.9\n",
      "Average loss at step 630:   1.9\n",
      "Average loss at step 640:   1.9\n",
      "Average loss at step 650:   1.9\n",
      "Average loss at step 660:   1.9\n",
      "Average loss at step 670:   1.9\n",
      "Average loss at step 680:   1.9\n",
      "Average loss at step 690:   1.9\n",
      "Average loss at step 700:   1.9\n",
      "Average loss at step 710:   1.9\n",
      "Average loss at step 720:   1.9\n",
      "Average loss at step 730:   1.9\n",
      "Average loss at step 740:   1.9\n",
      "Average loss at step 750:   1.9\n",
      "Average loss at step 760:   1.9\n",
      "Average loss at step 770:   1.9\n",
      "Average loss at step 780:   1.9\n",
      "Average loss at step 790:   1.9\n",
      "Average loss at step 800:   1.9\n",
      "Average loss at step 810:   1.9\n",
      "Average loss at step 820:   1.9\n",
      "Average loss at step 830:   1.9\n",
      "Average loss at step 840:   1.9\n",
      "Average loss at step 850:   1.9\n",
      "Average loss at step 860:   1.9\n",
      "Average loss at step 870:   1.9\n",
      "Average loss at step 880:   1.9\n",
      "Average loss at step 890:   1.9\n",
      "Average loss at step 900:   1.9\n",
      "Average loss at step 910:   1.9\n",
      "Average loss at step 920:   1.9\n",
      "Average loss at step 930:   1.9\n",
      "Average loss at step 940:   1.9\n",
      "Average loss at step 950:   1.9\n",
      "Average loss at step 960:   1.9\n",
      "Average loss at step 970:   1.9\n",
      "Average loss at step 980:   1.9\n",
      "Average loss at step 990:   1.9\n",
      "Average loss at step 1000:   1.9\n",
      "Average loss at step 1010:   1.9\n",
      "Average loss at step 1020:   1.9\n",
      "Average loss at step 1030:   1.9\n",
      "Average loss at step 1040:   1.9\n",
      "Average loss at step 1050:   1.9\n",
      "Average loss at step 1060:   1.9\n",
      "Average loss at step 1070:   1.9\n",
      "Average loss at step 1080:   1.9\n",
      "Average loss at step 1090:   1.9\n",
      "Average loss at step 1100:   1.9\n",
      "Average loss at step 1110:   1.9\n",
      "Average loss at step 1120:   1.9\n",
      "Average loss at step 1130:   1.9\n",
      "Average loss at step 1140:   1.9\n",
      "Average loss at step 1150:   1.9\n",
      "Average loss at step 1160:   1.9\n",
      "Average loss at step 1170:   1.9\n",
      "Average loss at step 1180:   1.9\n",
      "Average loss at step 1190:   1.9\n",
      "Average loss at step 1200:   1.9\n",
      "Average loss at step 1210:   1.9\n",
      "Average loss at step 1220:   1.9\n",
      "Average loss at step 1230:   1.9\n",
      "Average loss at step 1240:   1.9\n",
      "Average loss at step 1250:   1.9\n",
      "Average loss at step 1260:   1.9\n",
      "Average loss at step 1270:   1.9\n",
      "Average loss at step 1280:   1.9\n",
      "Accuracy 0.5667\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 #Define a batch size, it should be at the head of the program, not this cell,anyway\n",
    "NUM_EPO = 10\n",
    "SKIP_STEP = 10 # How many times we want to see our update, this means we want to see update every 10 batchs\n",
    "total_cost = 0\n",
    "init = tf.global_variables_initializer() \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)     #Init variables\n",
    "    \n",
    "    '''Start to init model'''\n",
    "    for index in range(BATCH_SIZE*NUM_EPO):\n",
    "        train_x, train_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch = sess.run([optimizer, cost], feed_dict={x: train_x, y:train_y}) #Doing optimization part\n",
    "        total_cost += loss_batch   # See the comment in the next 3 lines\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_cost / SKIP_STEP))\n",
    "            total_cost = 0.0       # Reset for next average\n",
    "    '''Finish training'''\n",
    "    \n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        loss_batch, logits_batch = sess.run([cost, y_pred], \n",
    "                                        feed_dict={x: X_batch, y:Y_batch}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        \n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))  # Count correct classification \n",
    "        \n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
