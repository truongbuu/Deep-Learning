{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will implement an autoencoder for MNIST dataset <br />\n",
    "Note that for MNIST dataset, the value of each pixel is binary(either 0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build a autoencoder with deep neural network\n",
    "\n",
    "def build_mlp(input_placeholder,scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        enc = tf.layers.dense(inputs = input_placeholder, units=128, activation= tf.nn.relu)\n",
    "        enc = tf.layers.dense(inputs = enc, units=128, activation= tf.nn.relu)\n",
    "        enc = tf.layers.dense(inputs = enc, units=64, activation= tf.nn.relu)\n",
    "        \n",
    "        dec = tf.layers.dense(inputs = enc, units=64, activation= tf.nn.tanh)\n",
    "        dec = tf.layers.dense(inputs = dec, units=128, activation= tf.nn.tanh)\n",
    "        dec = tf.layers.dense(inputs = dec, units=784, activation= tf.nn.tanh)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input tensor\n",
    "\n",
    "with tf.name_scope('input_data'):\n",
    "    x = tf.placeholder(tf.float32, shape = [None , 784]) #None means that it can be any arbitary\n",
    "with tf.name_scope('output_image'):\n",
    "    y = build_mlp(x,'aue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function:\n",
    "loss = tf.losses.mean_squared_error(x, y)\n",
    "#Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10:   0.1\n",
      "Average loss at step 20:   0.1\n",
      "Average loss at step 30:   0.1\n",
      "Average loss at step 40:   0.1\n",
      "Average loss at step 50:   0.1\n",
      "Average loss at step 60:   0.1\n",
      "Average loss at step 70:   0.0\n",
      "Average loss at step 80:   0.0\n",
      "Average loss at step 90:   0.0\n",
      "Average loss at step 100:   0.0\n",
      "Average loss at step 110:   0.0\n",
      "Average loss at step 120:   0.0\n",
      "Average loss at step 130:   0.0\n",
      "Average loss at step 140:   0.0\n",
      "Average loss at step 150:   0.0\n",
      "Average loss at step 160:   0.0\n",
      "Average loss at step 170:   0.0\n",
      "Average loss at step 180:   0.0\n",
      "Average loss at step 190:   0.0\n",
      "Average loss at step 200:   0.0\n",
      "Average loss at step 210:   0.0\n",
      "Average loss at step 220:   0.0\n",
      "Average loss at step 230:   0.0\n",
      "Average loss at step 240:   0.0\n",
      "Average loss at step 250:   0.0\n",
      "Average loss at step 260:   0.0\n",
      "Average loss at step 270:   0.0\n",
      "Average loss at step 280:   0.0\n",
      "Average loss at step 290:   0.0\n",
      "Average loss at step 300:   0.0\n",
      "Average loss at step 310:   0.0\n",
      "Average loss at step 320:   0.0\n",
      "Average loss at step 330:   0.0\n",
      "Average loss at step 340:   0.0\n",
      "Average loss at step 350:   0.0\n",
      "Average loss at step 360:   0.0\n",
      "Average loss at step 370:   0.0\n",
      "Average loss at step 380:   0.0\n",
      "Average loss at step 390:   0.0\n",
      "Average loss at step 400:   0.0\n",
      "Average loss at step 410:   0.0\n",
      "Average loss at step 420:   0.0\n",
      "Average loss at step 430:   0.0\n",
      "Average loss at step 440:   0.0\n",
      "Average loss at step 450:   0.0\n",
      "Average loss at step 460:   0.0\n",
      "Average loss at step 470:   0.0\n",
      "Average loss at step 480:   0.0\n",
      "Average loss at step 490:   0.0\n",
      "Average loss at step 500:   0.0\n",
      "Average loss at step 510:   0.0\n",
      "Average loss at step 520:   0.0\n",
      "Average loss at step 530:   0.0\n",
      "Average loss at step 540:   0.0\n",
      "Average loss at step 550:   0.0\n",
      "Average loss at step 560:   0.0\n",
      "Average loss at step 570:   0.0\n",
      "Average loss at step 580:   0.0\n",
      "Average loss at step 590:   0.0\n",
      "Average loss at step 600:   0.0\n",
      "Average loss at step 610:   0.0\n",
      "Average loss at step 620:   0.0\n",
      "Average loss at step 630:   0.0\n",
      "Average loss at step 640:   0.0\n",
      "Average loss at step 650:   0.0\n",
      "Average loss at step 660:   0.0\n",
      "Average loss at step 670:   0.0\n",
      "Average loss at step 680:   0.0\n",
      "Average loss at step 690:   0.0\n",
      "Average loss at step 700:   0.0\n",
      "Average loss at step 710:   0.0\n",
      "Average loss at step 720:   0.0\n",
      "Average loss at step 730:   0.0\n",
      "Average loss at step 740:   0.0\n",
      "Average loss at step 750:   0.0\n",
      "Average loss at step 760:   0.0\n",
      "Average loss at step 770:   0.0\n",
      "Average loss at step 780:   0.0\n",
      "Average loss at step 790:   0.0\n",
      "Average loss at step 800:   0.0\n",
      "Average loss at step 810:   0.0\n",
      "Average loss at step 820:   0.0\n",
      "Average loss at step 830:   0.0\n",
      "Average loss at step 840:   0.0\n",
      "Average loss at step 850:   0.0\n",
      "Average loss at step 860:   0.0\n",
      "Average loss at step 870:   0.0\n",
      "Average loss at step 880:   0.0\n",
      "Average loss at step 890:   0.0\n",
      "Average loss at step 900:   0.0\n",
      "Average loss at step 910:   0.0\n",
      "Average loss at step 920:   0.0\n",
      "Average loss at step 930:   0.0\n",
      "Average loss at step 940:   0.0\n",
      "Average loss at step 950:   0.0\n",
      "Average loss at step 960:   0.0\n",
      "Average loss at step 970:   0.0\n",
      "Average loss at step 980:   0.0\n",
      "Average loss at step 990:   0.0\n",
      "Average loss at step 1000:   0.0\n",
      "Average loss at step 1010:   0.0\n",
      "Average loss at step 1020:   0.0\n",
      "Average loss at step 1030:   0.0\n",
      "Average loss at step 1040:   0.0\n",
      "Average loss at step 1050:   0.0\n",
      "Average loss at step 1060:   0.0\n",
      "Average loss at step 1070:   0.0\n",
      "Average loss at step 1080:   0.0\n",
      "Average loss at step 1090:   0.0\n",
      "Average loss at step 1100:   0.0\n",
      "Average loss at step 1110:   0.0\n",
      "Average loss at step 1120:   0.0\n",
      "Average loss at step 1130:   0.0\n",
      "Average loss at step 1140:   0.0\n",
      "Average loss at step 1150:   0.0\n",
      "Average loss at step 1160:   0.0\n",
      "Average loss at step 1170:   0.0\n",
      "Average loss at step 1180:   0.0\n",
      "Average loss at step 1190:   0.0\n",
      "Average loss at step 1200:   0.0\n",
      "Average loss at step 1210:   0.0\n",
      "Average loss at step 1220:   0.0\n",
      "Average loss at step 1230:   0.0\n",
      "Average loss at step 1240:   0.0\n",
      "Average loss at step 1250:   0.0\n",
      "Average loss at step 1260:   0.0\n",
      "Average loss at step 1270:   0.0\n",
      "Average loss at step 1280:   0.0\n",
      "Model saved in file: saver/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 #Define a batch size, it should be at the head of the program, not this cell,anyway\n",
    "NUM_EPO = 10\n",
    "SKIP_STEP = 10\n",
    "init = tf.global_variables_initializer() \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    total_cost = 0\n",
    "    for i in range(BATCH_SIZE*NUM_EPO):\n",
    "        train_x, train_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch = sess.run([optimizer, loss], feed_dict={x: train_x})\n",
    "        total_cost += loss_batch\n",
    "        if (i + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(i + 1, total_cost / SKIP_STEP))\n",
    "            total_cost = 0.0       # Reset for next average\n",
    "    save_path = saver.save(sess, \"saver/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saver/model.ckpt\n",
      "Model restored.\n",
      "0.0355894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"saver/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    print (sess.run(loss, feed_dict={x: mnist.test.images}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saver/model.ckpt\n",
      "Model restored.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f059fb4ff10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "#Print several images and its reconstruction in the testset\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"saver/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    X,label = mnist.test.next_batch(1)\n",
    "    out_image = (sess.run(y, feed_dict={x: X}))\n",
    "    img = out_image>0.5\n",
    "    img = img.reshape(28,28)\n",
    "    X = X[0].reshape(28,28)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(X, cmap='gray')\n",
    "    plt.title('Origin')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title('Reconstructed')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
